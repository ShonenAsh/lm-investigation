# Language Model Investigation
DS 5500 - Data Science Capstone Project

## Summary

Large Language Models have taken the world by storm and are particularly prevalent in public online spaces such as social media. LLMs and their cousins, Small Language Models, are particularly prevalent in spam dedicated to harmful online discourse such as in comment sections of politically inflammatory topics. We propose an exploratory analysis to view public datasets of user generated content and compare user generated responses to synthetic AI generated responses. To achieve this, we conduct a statistical analysis to find meaningful differentiation patterns and potentially find exploits that help us quickly identify synthetic posts and the popular language model architectures behind them.

## Stylometric Analysis
- AI vs human text: Difference in parts of speech distributions
<img width="1466" height="965" alt="image" src="https://github.com/user-attachments/assets/7f10f208-85fc-4d0c-88ed-3b0d3aafa3c7" />

- AI vs human text: Number of unique words (Type-Token Ratio) on r/Cornell posts
<img width="712" height="575" alt="image" src="https://github.com/user-attachments/assets/c7f7053a-53e9-4fb7-84c2-50fb94d0efa4" />


## Biber features
Douglas Biber is a Linguist, Professor at Arizona State University who dedicated his career toward researching and studying linguistic variations and writing styles observed in English corpora. In 1988, and the years following, he developed a framework to analyze linguistic styles used in English grouped across 6 major dimensions [^1][^2]. 
We use this framework to assess the 67 features present in Biber's original work individually to screen for statistical significance. Biber features are calculated based on parts of speech, semantic meanings of specific word types, and word dependencies. To achieve this we use the Python library "PyBiber" which uses relations generated by the
SpaCy library and extracts feature frequencies scaled to be normalized to a 1,000 word corpus.

- Biber features ranked based on separability across multiple classifiers
<img width="1250" height="990" alt="image" src="https://github.com/user-attachments/assets/06238811-5966-4489-ba4e-6026529f855c" />

- Classification results (Other scenario results can be found under notebooks/biber_features_analysis/models.ipynb)
<img width="790" height="790" alt="image" src="https://github.com/user-attachments/assets/dd19ca3e-af6e-479b-b1b9-09b7a7fd6772" />

## Observations
-  We see that AI's distributions are unlike what humans' typically type in an online discussion board like reddit.
-  Biber feature analysis suggests that Language models tend to
    - use more contractions (e.g. I'm, can't, could've)
    - have lesser type token ratios, owing to their verbosity
    - use more private verbs (verbs describing internal mental states e.g. "consider", "forget")
    - attributive adjectives (e.g. <b>lovely</b> weather) as opposed to predicative adjectives (e.g. The weather is <b>lovely</b>>) 
-  AI-generated text is generally harder to read for an average reader in the US (based on Flesch-Kincade reading scale).


### References
- [^1] Douglas Biber. Dimensions of register variation: A cross-linguistic comparison, 1995
- [^2] [Biber's MDA Case Studies](https://jan.ucc.nau.edu/biber/Biber/Biber_Finegan_1994.pdf)
